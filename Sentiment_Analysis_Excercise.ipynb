{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Excercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkj0dKIcw/rjSqTRhEtT9I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raynerz/nlp/blob/main/Sentiment_Analysis_Excercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfR3Tu6O5GgX"
      },
      "source": [
        "Download the dataset to train your system:\n",
        "http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "Write a Python Script to:\n",
        "- Read training and test data from the files\n",
        "- Preprocess the data (e.g. replace unwanted characters with space, or remove\n",
        "html expressions)\n",
        "- Lemmatize the test and training data\n",
        "- Use Vectorization to get a numeric representation (for example by using\n",
        "CountVectorizer*)\n",
        "- Use a LogisticRegression classifier to train the model\n",
        "- Hint: You can use a Scikit-learn Pipeline as seen previsouly this semester\n",
        "\n",
        "Sources: https://towardsdatascience.com/a-complete-sentiment-analysis-algorithm-in-python-with-amazon-product-review-data-step-by-step-2680d2e2c23b\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ1bi3BlMAJ_",
        "outputId": "71204c0e-afa8-4734-b036-01207afcea50"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('names')\n",
        "!pip install normalise\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import lxml.html.clean as clean\n",
        "import string\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from normalise import normalise\n",
        "import en_core_web_sm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "Requirement already satisfied: normalise in /usr/local/lib/python3.7/dist-packages (0.1.8)\n",
            "Requirement already satisfied: roman in /usr/local/lib/python3.7/dist-packages (from normalise) (3.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from normalise) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from normalise) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from normalise) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from normalise) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->normalise) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->normalise) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_0Ja9J946c6"
      },
      "source": [
        "%%capture \n",
        "# Capture stops Jupyter from outputs\n",
        "\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xvf aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTB6al-86quQ",
        "outputId": "43d51b1e-8ab4-49b3-bea0-571ed5887d34"
      },
      "source": [
        "# Read Data\n",
        "\n",
        "\n",
        "def read_data(path=\"\", sentiment=True): #True is positive, False is negative\n",
        "  directorylist = os.listdir(path) # Reading data from the system\n",
        "  data = []\n",
        "\n",
        "  for i in directorylist:\n",
        "    rating = i.split(\"_\")[1]\n",
        "    rating = rating.split(\".\")[0]\n",
        "    rating = int(rating)\n",
        "\n",
        "    f = open(path+i)\n",
        "    text = f.read()\n",
        "    data.append((text, rating, sentiment))\n",
        "  return data\n",
        "    \n",
        "  \n",
        "\n",
        "reviews_neg = read_data(\"./aclImdb/train/neg/\", False)\n",
        "reviews_pos = read_data(\"./aclImdb/train/pos/\", True)\n",
        "\n",
        "# Prepare data for the model\n",
        "\n",
        "reviews_neg = pd.DataFrame(reviews_neg).rename(columns={0:\"text\", 1:\"rating\", 2:\"sentiment\"})\n",
        "reviews_pos = pd.DataFrame(reviews_pos).rename(columns={0:\"text\", 1:\"rating\", 2:\"sentiment\"})\n",
        "\n",
        "all_reviews = pd.concat([reviews_neg, reviews_pos])\n",
        "all_reviews = shuffle(all_reviews)\n",
        "\n",
        "print(all_reviews)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    text  rating  sentiment\n",
            "9738   **Warning! Mild Spoilers Ahead!**<br /><br />(...      10       True\n",
            "8853   Relentlessly stupid, no-budget \"war picture\" m...       3      False\n",
            "4440   Before this, the flawed \"Slaughterhouse Five\" ...      10       True\n",
            "3618   No movie with Madeleine Carroll in its cast co...       2      False\n",
            "8943   I'm glad the folks at IMDb were able to deciph...       1      False\n",
            "...                                                  ...     ...        ...\n",
            "11863  This film is a complete re-imagining of Romeo ...      10       True\n",
            "11499  I have spent the last week watching John Cassa...      10       True\n",
            "8412   Dr Tarr's Torture Dungeon is about a journalis...       4      False\n",
            "4999   A bit slow (somehow like a Sofia Coppola movie...       8       True\n",
            "9575   Good lord, whoever made this turkey needs to b...       2      False\n",
            "\n",
            "[25000 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hrfSMAmaNqZ6",
        "outputId": "28aaf7a8-98e8-4126-dc2a-dec1fc244034"
      },
      "source": [
        "# Cleaning: Eliminating punctuation symbols or other unwanted symbols like html\n",
        "\n",
        "\n",
        "\n",
        "tokens = []\n",
        "for text in all_reviews['text']:\n",
        "  text = clean.clean_html(text)\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  tokens.append(text)\n",
        "\n",
        "df = pd.DataFrame(tokens)\n",
        "all_reviews['text'] = df[0].values\n",
        "all_reviews.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>rating</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9738</th>\n",
              "      <td>pWarning Mild Spoilers AheadbrbrYes I realize ...</td>\n",
              "      <td>10</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8853</th>\n",
              "      <td>pRelentlessly stupid nobudget war picture made...</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4440</th>\n",
              "      <td>pBefore this the flawed Slaughterhouse Five wa...</td>\n",
              "      <td>10</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3618</th>\n",
              "      <td>pNo movie with Madeleine Carroll in its cast c...</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8943</th>\n",
              "      <td>pIm glad the folks at IMDb were able to deciph...</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  rating  sentiment\n",
              "9738  pWarning Mild Spoilers AheadbrbrYes I realize ...      10       True\n",
              "8853  pRelentlessly stupid nobudget war picture made...       3      False\n",
              "4440  pBefore this the flawed Slaughterhouse Five wa...      10       True\n",
              "3618  pNo movie with Madeleine Carroll in its cast c...       2      False\n",
              "8943  pIm glad the folks at IMDb were able to deciph...       1      False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ-D-zaYZes0"
      },
      "source": [
        "## Pipeline for\n",
        "1. Normaliting\n",
        "2. Remove Punctuation\n",
        "3. Remove Stop words\n",
        "4. Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV3aqzQ2PRNu"
      },
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "      normalized_text = normalize(text)\n",
        "      doc = nlp(normalized_text)\n",
        "      removed_punct = remove_punct(doc)\n",
        "      removed_stop_words = remove_stop_words(removed_punct)\n",
        "      return lemmatize(removed_stop_words)\n",
        "\n",
        "def normalize(text):\n",
        "    # some issues in normalise package\n",
        "    try:\n",
        "        return ' '.join(normalise(text, verbose=False))\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "def remove_punct(doc):\n",
        "    return [t for t in doc if t.text not in string.punctuation]\n",
        "\n",
        "def remove_stop_words(doc):\n",
        "    return [t for t in doc if not t.is_stop]\n",
        "\n",
        "def lemmatize(doc):\n",
        "    return ' '.join([t.lemma_ for t in doc])\n",
        "\n",
        "preprocessed =  []\n",
        "for text in all_reviews['text']:\n",
        "  result = preprocess_text(text)\n",
        "  preprocessed.append(result)\n",
        "\n",
        "df = pd.DataFrame(preprocessed)\n",
        "\n",
        "all_reviews['text'] = df[0].values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLt_oelE6lfo"
      },
      "source": [
        "# Split into training and validation set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = all_reviews['text']\n",
        "y = all_reviews['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prbNSYIPHDnZ"
      },
      "source": [
        "# CountVectorizer develops a vector of all the words in the string. \n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "\n",
        "ctmTr = cv.fit_transform(X_train)\n",
        "X_test_dtm = cv.transform(X_test)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtJQdWNhHQ1z",
        "outputId": "464261b5-d7c5-4055-9590-0e98daafeaff"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "model = LogisticRegression()\n",
        "model.fit(ctmTr, y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGbjf2rLI_Ts"
      },
      "source": [
        "# Predict\n",
        "\n",
        "y_pred_class = model.predict(X_test_dtm)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_C1Pz5mJPD9",
        "outputId": "99db0b76-4762-4aa7-86b3-580c613d5f9c"
      },
      "source": [
        "# Evaluate\n",
        "\n",
        "accuracy_score(y_test, y_pred_class)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.86656"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80YoD5izJT6i"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}